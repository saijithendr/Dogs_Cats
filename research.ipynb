{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shutil \n",
    "* Shutil module offers high-level operation on a file like a copy, create, and remote operation on the file. It comes under Python's standard utility modules. This module helps in automating the process of copying and removal of files and directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "train_dir = os.path.join('data', 'cats_dogs', 'PetImages', 'train')\n",
    "test_dir = os.path.join('data', 'cats_dogs', 'PetImages', 'test')\n",
    "\n",
    "transform = T.Compose([T.Resize((150, 200)),T.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root=train_dir, transform=transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=10, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)     # 3-color, output-6, kernal-5\n",
    "        self.pool = nn.MaxPool2d(2, 2)      # pooling---> kernal-2 maxpool-2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 4)    # Conv---> 6-input(must be same as output of prev conv), output-16, kernal-5\n",
    "        self.fcn1 = nn.Linear(19600, 120)  # fully NN1---> input-16*5*5, hidden-120    32-5+\n",
    "        self.fcn2 = nn.Linear(120, 90)      # fully NN2---> hidden_input-120, hidden_output-80 \n",
    "        self.fcn3 = nn.Linear(90, 2)       # fully NN3---> input-80, output-2 (num target classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        print(f'conv1->relu and pool shape:{x.shape}')\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        print(f'conv2 -> relu and pool shape: {x.shape}')\n",
    "        x = x.view(-1, 16*35*35)\n",
    "        print(f'After reshaping shape: {x.shape}')\n",
    "        x = F.relu(self.fcn1(x))\n",
    "        print(f'fc1 shape: {x.shape}')\n",
    "        x = F.relu(self.fcn2(x))\n",
    "        print(f'fc2 shape: {x.shape}')\n",
    "        x = self.fcn3(x)\n",
    "        return x\n",
    "    \n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n",
      "conv1->relu and pool shape:torch.Size([32, 6, 73, 73])\n",
      "conv2 -> relu and pool shape: torch.Size([32, 16, 35, 35])\n",
      "After reshaping shape: torch.Size([32, 19600])\n",
      "fc1 shape: torch.Size([32, 120])\n",
      "fc2 shape: torch.Size([32, 90])\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file <_io.BufferedReader name='data/cats_dogs/PetImages\\\\train\\\\Cat\\\\666.jpg'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m#Loop \u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, (img, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_data):\n\u001b[0;32m     59\u001b[0m         img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     60\u001b[0m         label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[39m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloader(path)\n\u001b[0;32m    230\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[39mreturn\u001b[39;00m pil_loader(path)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpil_loader\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Image\u001b[39m.\u001b[39mImage:\n\u001b[0;32m    245\u001b[0m     \u001b[39m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m    246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m--> 247\u001b[0m         img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(f)\n\u001b[0;32m    248\u001b[0m         \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39mconvert(\u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\PIL\\Image.py:3186\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3184\u001b[0m \u001b[39mfor\u001b[39;00m message \u001b[39min\u001b[39;00m accept_warnings:\n\u001b[0;32m   3185\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(message)\n\u001b[1;32m-> 3186\u001b[0m \u001b[39mraise\u001b[39;00m UnidentifiedImageError(\n\u001b[0;32m   3187\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcannot identify image file \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (filename \u001b[39mif\u001b[39;00m filename \u001b[39melse\u001b[39;00m fp)\n\u001b[0;32m   3188\u001b[0m )\n",
      "\u001b[1;31mUnidentifiedImageError\u001b[0m: cannot identify image file <_io.BufferedReader name='data/cats_dogs/PetImages\\\\train\\\\Cat\\\\666.jpg'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def load_data(train_dir: str, test_dir: str, batch_size: int):\n",
    "    \n",
    "    transform = T.Compose(\n",
    "        [T.Resize((150, 150), T.InterpolationMode.BILINEAR), T.ToTensor()]\n",
    "    )\n",
    "    \n",
    "    train_dataset = torchvision.datasets.ImageFolder(\n",
    "        root= train_dir, transform= transform\n",
    "    )\n",
    "    \n",
    "    test_dataset =  torchvision.datasets.ImageFolder(\n",
    "        root= test_dir, transform= transform\n",
    "    )\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "\n",
    "train_dir = os.path.join('data/cats_dogs/PetImages', 'train')\n",
    "test_dir = os.path.join('data/cats_dogs/PetImages', 'test')\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "\n",
    "train_data, test_data = load_data(\n",
    "    train_dir=train_dir, test_dir=test_dir, batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Training \n",
    "model = CNN().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=learning_rate )\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "n_total_steps = len(train_data)\n",
    "\n",
    "#Loop \n",
    "for epoch in range(num_epochs):\n",
    "    for id, (img, label) in enumerate(train_data):\n",
    "        \n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(img)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        #Backward and loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        if (id+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {id+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fibonacci\n",
    "\n",
    "def fib(n):\n",
    "    if n==1:\n",
    "        return 0\n",
    "    elif n==2:\n",
    "        return 1\n",
    "    else:\n",
    "        return fib(n-1) + fib(n-2)\n",
    "    \n",
    "fib(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onetoN(n):\n",
    "    if n == 1:\n",
    "        print(1, end=',')\n",
    "        return\n",
    "    print(n, end=',')\n",
    "    onetoN(n-1)\n",
    "\n",
    "onetoN(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# factorial\n",
    "def factorial(n):\n",
    "    if n==1:\n",
    "        return n*1\n",
    "    elif n == 0:\n",
    "        return 0\n",
    "    return n*factorial(n-1)\n",
    "\n",
    "factorial(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "240bc028caeb8b02ff80d8aedfc61caf7a0e4db2770780d40c5b717508bae340"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
